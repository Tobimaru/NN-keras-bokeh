{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One dimensional example of GAN\n",
    "\n",
    "This is still work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_params = {\n",
    "    'mean': 1.0,\n",
    "    'sigma': 2.0, \n",
    "    'noise_bound': 8.0\n",
    "}\n",
    "\n",
    "generator_hidden_dims = 4\n",
    "model_params = {\n",
    "    # generator hidden layer size\n",
    "    'gen_hid_size': generator_hidden_dims,\n",
    "    # discriminator needs to be more powerful\n",
    "    # than the generator in this case\n",
    "    'discrim_hid_size': 2 * generator_hidden_dims,\n",
    "    'kernel_init': 'random_normal',\n",
    "    'batch_size': 5,\n",
    "    'optim': 'Adam',\n",
    "    'loss': 'binary_crossentropy'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data sampling\n",
    "\n",
    "define data sampling functions:  \n",
    "**sample_real**: sampling of real data from a Gaussian distribution with given sigma and mean  \n",
    "**sample_noise**: sampling of noise as the generator distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real(n, sigma=1.0, mean=0.0):\n",
    "    return np.sort(np.random.normal(mean, sigma, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(n, bound=8.0):\n",
    "    #use stratified sampling\n",
    "    return np.linspace(-bound, bound, n) + \\\n",
    "            np.random.random(n) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define generator\n",
    "\n",
    "Job of generator is to generate synthetic data to mimic real data.  \n",
    "In this case we use a neural network to simulate the inverse of  \n",
    "the cumulative density function of a Gaussian Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inputs, hidden_dim, init): \n",
    "    # first linear transform without activation\n",
    "    x = Dense(hidden_dim, kernel_initializer=init, \n",
    "              activation='softplus')(inputs)\n",
    "    # apply nonlinear transform using softplus\n",
    "    return Dense(1, kernel_initializer=init)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, hidden_dim, init):\n",
    "    activ_func = 'relu'\n",
    "    x = Dense(hidden_dim, kernel_initializer=init, \n",
    "              activation=activ_func)(inputs)\n",
    "    for i in range(2):\n",
    "        x = Dense(hidden_dim, kernel_initializer=init, \n",
    "                  activation=activ_func)(x)     \n",
    "    return Dense(1, kernel_initializer=init,\n",
    "                        activation='sigmoid')(x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the GAN model\n",
    "\n",
    "The model will use binary cross entropy as loss function. The targets will depend on the training phase. Essentially we want to achieve:\n",
    "* First, we train the discriminator to distinguish between the real and fake inputs. The generator is kept fixed, and we use both real and fake (from generator) inputs in the loss function. Target of the real and fake inputs will be one and zero respectively.\n",
    "* Second, we train the generator to mimic the real inputs. The discriminator is kept fixed, and we only consider the fake inputs from the generator in the loss function. Target of the fake inputs will be one.\n",
    "\n",
    "Apparently we cannot freeze layers **after** compilation in Keras. So we will need to create two models using the same layers, but freezing them differently for both training phases. Otherwise, we would have to recompile for each batch iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_as_discriminator(generator, discriminator):\n",
    "    out_fake = Activation('linear')(\n",
    "                    discriminator(generator(generator.inputs)))\n",
    "    out_real = Activation('linear')(\n",
    "                    discriminator(discriminator.inputs))\n",
    "    for layer in generator.layers:\n",
    "        layer.trainable=False\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable=True\n",
    "    #inputs of models are lists\n",
    "    return Model(inputs=generator.inputs + discriminator.inputs,\n",
    "                 outputs=[out_fake, out_real])\n",
    "\n",
    "def gan_as_generator(generator, discriminator):\n",
    "    out_fake = Activation('linear')(\n",
    "                    discriminator(generator(generator.inputs)))\n",
    "    for layer in generator.layers:\n",
    "        layer.trainable=True\n",
    "    for layer in discriminator.layers:\n",
    "        layer.trainable=False\n",
    "    return Model(inputs=generator.inputs, outputs=out_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, params):\n",
    "        self.batch_size = params['batch_size']\n",
    "        # initialize generator\n",
    "        self.gen_input = Input(shape=(1,), dtype='float32', \n",
    "                               name='gen_input')\n",
    "        self.generator = Model(inputs=self.gen_input, \n",
    "                               outputs=generator(self.gen_input, \n",
    "                                           params['gen_hid_size'],\n",
    "                                           params['kernel_init']))\n",
    "        # initialize discriminator\n",
    "        self.discrim_input = Input(shape=(1,), dtype='float32', \n",
    "                                   name='discrim_input')\n",
    "        self.discriminator = Model(inputs=self.discrim_input, \n",
    "                                   outputs=discriminator(\n",
    "                                               self.discrim_input,\n",
    "                                               params['discrim_hid_size'],\n",
    "                                               params['kernel_init']))  \n",
    "        # initialize gan version for training generator\n",
    "        self.gan_gen = gan_as_generator(self.generator, \n",
    "                                        self.discriminator)\n",
    "        # initialize gan version for training discriminator\n",
    "        self.gan_discrim = gan_as_discriminator(self.generator, \n",
    "                                                self.discriminator)\n",
    "        # compile the models\n",
    "        for model in [self.generator, self.discriminator,\n",
    "                      self.gan_gen, self.gan_discrim]:\n",
    "            model.compile(optimizer=params['optim'], \n",
    "                               loss=params['loss']) \n",
    "        \n",
    "        \n",
    "    def train_discriminator(self, params):\n",
    "        # generate batch of real and fake data\n",
    "        real_data = sample_real(self.batch_size,\n",
    "                                params['sigma'], \n",
    "                                params['mean'])\n",
    "        noise_data = sample_noise(self.batch_size, \n",
    "                                  params['noise_bound'])\n",
    "        # define target labels for real and noise data\n",
    "        real_target = np.ones(real_data.shape)\n",
    "        fake_target = np.zeros(noise_data.shape)\n",
    "        self.gan_discrim.fit(x=[real_data, noise_data], \n",
    "                             y=[real_target, fake_target],\n",
    "                             batch_size=self.batch_size)\n",
    "        \n",
    "     \n",
    "    def train_generator(self, params):\n",
    "        noise_data = sample_noise(self.batch_size, \n",
    "                                  params['noise_bound'])\n",
    "        target = np.ones(noise_data.shape)\n",
    "        \n",
    "        print(type(noise_data[0]))\n",
    "        print(type(target[0]))\n",
    "        \n",
    "        self.gan_gen.fit(x=noise_data, y=target, \n",
    "                         batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(model_params)\n",
    "\n",
    "for i in range(10):\n",
    "    gan.train_discriminator(problem_params)\n",
    "    gan.train_generator(problem_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
